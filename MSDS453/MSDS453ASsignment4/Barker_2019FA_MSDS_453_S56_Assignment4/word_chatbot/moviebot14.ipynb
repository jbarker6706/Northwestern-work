{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from tensorflow.keras import layers , activations , models , preprocessing\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB SIZE : 7159\n"
     ]
    }
   ],
   "source": [
    "r\"\"\" Build a word sequence-to-sequence training set \"\"\"\n",
    "\n",
    "from tensorflow.keras import preprocessing , utils\n",
    "\n",
    "questions = list()\n",
    "answers = list()\n",
    "\n",
    "with open(\"movies-sequence-input.txt\", \"r\") as file_input:\n",
    "    movie_input = file_input.read()\n",
    "df_input = pd.DataFrame(movie_input.split('\\n'),columns=list('i'))\n",
    "df_input = df_input.fillna(' ')\n",
    "\n",
    "with open(\"movies-sequence-output.txt\", \"r\") as file_output:\n",
    "    movie_output = file_output.read()\n",
    "df_output = pd.DataFrame(movie_output.split('\\n'),columns=list('o'))\n",
    "df_output = df_output.fillna(' ')\n",
    "\n",
    "for input_text, target_text in zip(df_input.i, df_output.o):\n",
    "    if(len(input_text)>400):\n",
    "        questions.append(input_text[:400])\n",
    "    else:\n",
    "        questions.append(input_text)\n",
    "    if(len(target_text)>400):\n",
    "        answers.append(target_text[:400])\n",
    "    else:\n",
    "        answers.append(target_text)\n",
    "answers_with_tags = list()\n",
    "for i in range( len( answers ) ):\n",
    "    if type( answers[i] ) == str:\n",
    "        answers_with_tags.append( answers[i] )\n",
    "    else:\n",
    "        questions.pop( i )\n",
    "\n",
    "answers = list()\n",
    "for i in range( len( answers_with_tags ) ) :\n",
    "    answers.append( '<START> ' + answers_with_tags[i] + ' <END>' )\n",
    "\n",
    "tokenizer = preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts( questions + answers )\n",
    "VOCAB_SIZE = len( tokenizer.word_index )+1\n",
    "print( 'VOCAB SIZE : {}'.format( VOCAB_SIZE ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6236, 83) 83\n",
      "(6236, 89) 89\n",
      "(6236, 89, 7159)\n"
     ]
    }
   ],
   "source": [
    "# encoder_input_data\n",
    "tokenized_questions = tokenizer.texts_to_sequences( questions )\n",
    "maxlen_questions = max( [ len(x) for x in tokenized_questions ] )\n",
    "padded_questions = preprocessing.sequence.pad_sequences( tokenized_questions , maxlen=maxlen_questions , padding='post' )\n",
    "encoder_input_data = np.array( padded_questions )\n",
    "print( encoder_input_data.shape , maxlen_questions )\n",
    "\n",
    "# decoder_input_data\n",
    "tokenized_answers = tokenizer.texts_to_sequences( answers )\n",
    "maxlen_answers = max( [ len(x) for x in tokenized_answers ] )\n",
    "padded_answers = preprocessing.sequence.pad_sequences( tokenized_answers , maxlen=maxlen_answers , padding='post' )\n",
    "decoder_input_data = np.array( padded_answers )\n",
    "print( decoder_input_data.shape , maxlen_answers )\n",
    "\n",
    "# decoder_output_data\n",
    "tokenized_answers = tokenizer.texts_to_sequences( answers )\n",
    "for i in range(len(tokenized_answers)) :\n",
    "    tokenized_answers[i] = tokenized_answers[i][1:]\n",
    "padded_answers = preprocessing.sequence.pad_sequences( tokenized_answers , maxlen=maxlen_answers , padding='post' )\n",
    "onehot_answers = utils.to_categorical( padded_answers , VOCAB_SIZE )\n",
    "decoder_output_data = np.array( onehot_answers )\n",
    "print( decoder_output_data.shape )\n",
    "\n",
    "# Saving all the arrays to storage\n",
    "np.save( 'enc_in_data.npy' , encoder_input_data )\n",
    "np.save( 'dec_in_data.npy' , decoder_input_data )\n",
    "np.save( 'dec_tar_data.npy' , decoder_output_data )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\jbark\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, None, 200)    1431800     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 200)    1431800     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 200), (None, 320800      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 200),  320800      embedding_1[0][0]                \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 7159)   1438959     lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 4,944,159\n",
      "Trainable params: 4,944,159\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_inputs = tf.keras.layers.Input(shape=( None , ))\n",
    "encoder_embedding = tf.keras.layers.Embedding( VOCAB_SIZE, 200 , mask_zero=True ) (encoder_inputs)\n",
    "encoder_outputs , state_h , state_c = tf.keras.layers.LSTM( 200 , return_state=True )( encoder_embedding )\n",
    "encoder_states = [ state_h , state_c ]\n",
    "\n",
    "decoder_inputs = tf.keras.layers.Input(shape=( None ,  ))\n",
    "decoder_embedding = tf.keras.layers.Embedding( VOCAB_SIZE, 200 , mask_zero=True) (decoder_inputs)\n",
    "decoder_lstm = tf.keras.layers.LSTM( 200 , return_state=True , return_sequences=True )\n",
    "decoder_outputs , _ , _ = decoder_lstm ( decoder_embedding , initial_state=encoder_states )\n",
    "decoder_dense = tf.keras.layers.Dense( VOCAB_SIZE , activation=tf.keras.activations.softmax ) \n",
    "output = decoder_dense ( decoder_outputs )\n",
    "\n",
    "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], output )\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='categorical_crossentropy')\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\jbark\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From C:\\Users\\jbark\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/100\n",
      "6236/6236 [==============================] - 184s 30ms/sample - loss: 5.9553\n",
      "Epoch 2/100\n",
      "6236/6236 [==============================] - 188s 30ms/sample - loss: 5.3526\n",
      "Epoch 3/100\n",
      "6236/6236 [==============================] - 189s 30ms/sample - loss: 5.1592\n",
      "Epoch 4/100\n",
      "6236/6236 [==============================] - 188s 30ms/sample - loss: 5.0224\n",
      "Epoch 5/100\n",
      "6236/6236 [==============================] - 187s 30ms/sample - loss: 4.8988\n",
      "Epoch 6/100\n",
      "6236/6236 [==============================] - 180s 29ms/sample - loss: 4.7901\n",
      "Epoch 7/100\n",
      "6236/6236 [==============================] - 184s 29ms/sample - loss: 4.6928\n",
      "Epoch 8/100\n",
      "6236/6236 [==============================] - 188s 30ms/sample - loss: 4.6078\n",
      "Epoch 9/100\n",
      "6236/6236 [==============================] - 189s 30ms/sample - loss: 4.5262\n",
      "Epoch 10/100\n",
      "6236/6236 [==============================] - 188s 30ms/sample - loss: 4.4515\n",
      "Epoch 11/100\n",
      "6236/6236 [==============================] - 182s 29ms/sample - loss: 4.3798\n",
      "Epoch 12/100\n",
      "6236/6236 [==============================] - 181s 29ms/sample - loss: 4.3058\n",
      "Epoch 13/100\n",
      "6236/6236 [==============================] - 186s 30ms/sample - loss: 4.2354\n",
      "Epoch 14/100\n",
      "6236/6236 [==============================] - 183s 29ms/sample - loss: 4.1613\n",
      "Epoch 15/100\n",
      "6236/6236 [==============================] - 187s 30ms/sample - loss: 4.0899\n",
      "Epoch 16/100\n",
      "6236/6236 [==============================] - 187s 30ms/sample - loss: 4.0269\n",
      "Epoch 17/100\n",
      "6236/6236 [==============================] - 181s 29ms/sample - loss: 3.9640\n",
      "Epoch 18/100\n",
      "6236/6236 [==============================] - 189s 30ms/sample - loss: 3.8982\n",
      "Epoch 19/100\n",
      "6236/6236 [==============================] - 187s 30ms/sample - loss: 3.8315\n",
      "Epoch 20/100\n",
      "6236/6236 [==============================] - 186s 30ms/sample - loss: 3.7663\n",
      "Epoch 21/100\n",
      "6236/6236 [==============================] - 189s 30ms/sample - loss: 3.7043\n",
      "Epoch 22/100\n",
      "6236/6236 [==============================] - 182s 29ms/sample - loss: 3.6318\n",
      "Epoch 23/100\n",
      "6236/6236 [==============================] - 187s 30ms/sample - loss: 3.5697\n",
      "Epoch 24/100\n",
      "6236/6236 [==============================] - 187s 30ms/sample - loss: 3.4997\n",
      "Epoch 25/100\n",
      "6236/6236 [==============================] - 187s 30ms/sample - loss: 3.4318\n",
      "Epoch 26/100\n",
      "6236/6236 [==============================] - 186s 30ms/sample - loss: 3.3700\n",
      "Epoch 27/100\n",
      "6236/6236 [==============================] - 178s 29ms/sample - loss: 3.3101\n",
      "Epoch 28/100\n",
      "6236/6236 [==============================] - 185s 30ms/sample - loss: 3.2424\n",
      "Epoch 29/100\n",
      "6236/6236 [==============================] - 190s 30ms/sample - loss: 3.1824\n",
      "Epoch 30/100\n",
      "6236/6236 [==============================] - 187s 30ms/sample - loss: 3.1190\n",
      "Epoch 31/100\n",
      "6236/6236 [==============================] - 187s 30ms/sample - loss: 3.0554\n",
      "Epoch 32/100\n",
      "6236/6236 [==============================] - 181s 29ms/sample - loss: 2.9904\n",
      "Epoch 33/100\n",
      "6236/6236 [==============================] - 186s 30ms/sample - loss: 2.9321\n",
      "Epoch 34/100\n",
      "6236/6236 [==============================] - 186s 30ms/sample - loss: 2.8702\n",
      "Epoch 35/100\n",
      "6236/6236 [==============================] - 184s 30ms/sample - loss: 2.8111\n",
      "Epoch 36/100\n",
      "6236/6236 [==============================] - 186s 30ms/sample - loss: 2.7480\n",
      "Epoch 37/100\n",
      "6236/6236 [==============================] - 185s 30ms/sample - loss: 2.6934\n",
      "Epoch 38/100\n",
      "6236/6236 [==============================] - 181s 29ms/sample - loss: 2.6296\n",
      "Epoch 39/100\n",
      "6236/6236 [==============================] - 186s 30ms/sample - loss: 2.5793\n",
      "Epoch 40/100\n",
      "6236/6236 [==============================] - 187s 30ms/sample - loss: 2.5262\n",
      "Epoch 41/100\n",
      "6236/6236 [==============================] - 187s 30ms/sample - loss: 2.4702\n",
      "Epoch 42/100\n",
      "6236/6236 [==============================] - 186s 30ms/sample - loss: 2.4167\n",
      "Epoch 43/100\n",
      "6236/6236 [==============================] - 189s 30ms/sample - loss: 2.3677\n",
      "Epoch 44/100\n",
      "6236/6236 [==============================] - 177s 28ms/sample - loss: 2.3124\n",
      "Epoch 45/100\n",
      "6236/6236 [==============================] - 185s 30ms/sample - loss: 2.2645\n",
      "Epoch 46/100\n",
      "6236/6236 [==============================] - 189s 30ms/sample - loss: 2.2140\n",
      "Epoch 47/100\n",
      "6236/6236 [==============================] - 189s 30ms/sample - loss: 2.1692\n",
      "Epoch 48/100\n",
      "6236/6236 [==============================] - 189s 30ms/sample - loss: 2.1120\n",
      "Epoch 49/100\n",
      "6236/6236 [==============================] - 183s 29ms/sample - loss: 2.0654\n",
      "Epoch 50/100\n",
      "6236/6236 [==============================] - 187s 30ms/sample - loss: 2.0184\n",
      "Epoch 51/100\n",
      "6236/6236 [==============================] - 191s 31ms/sample - loss: 1.9787\n",
      "Epoch 52/100\n",
      "6236/6236 [==============================] - 186s 30ms/sample - loss: 1.9297\n",
      "Epoch 53/100\n",
      "6236/6236 [==============================] - 191s 31ms/sample - loss: 1.8917\n",
      "Epoch 54/100\n",
      "6236/6236 [==============================] - 189s 30ms/sample - loss: 1.8437\n",
      "Epoch 55/100\n",
      "6236/6236 [==============================] - 180s 29ms/sample - loss: 1.8006\n",
      "Epoch 56/100\n",
      "6236/6236 [==============================] - 190s 30ms/sample - loss: 1.7603\n",
      "Epoch 57/100\n",
      "6236/6236 [==============================] - 189s 30ms/sample - loss: 1.7201\n",
      "Epoch 58/100\n",
      "6236/6236 [==============================] - 191s 31ms/sample - loss: 1.6759\n",
      "Epoch 59/100\n",
      "6236/6236 [==============================] - 190s 31ms/sample - loss: 1.6357\n",
      "Epoch 60/100\n",
      "6236/6236 [==============================] - 179s 29ms/sample - loss: 1.6024\n",
      "Epoch 61/100\n",
      "6236/6236 [==============================] - 188s 30ms/sample - loss: 1.5613\n",
      "Epoch 62/100\n",
      "6236/6236 [==============================] - 190s 30ms/sample - loss: 1.5225\n",
      "Epoch 63/100\n",
      "6236/6236 [==============================] - 186s 30ms/sample - loss: 1.4853\n",
      "Epoch 64/100\n",
      "6236/6236 [==============================] - 187s 30ms/sample - loss: 1.4535\n",
      "Epoch 65/100\n",
      "6236/6236 [==============================] - 182s 29ms/sample - loss: 1.4187\n",
      "Epoch 66/100\n",
      "6236/6236 [==============================] - 186s 30ms/sample - loss: 1.3763\n",
      "Epoch 67/100\n",
      "6236/6236 [==============================] - 188s 30ms/sample - loss: 1.3470\n",
      "Epoch 68/100\n",
      "6236/6236 [==============================] - 187s 30ms/sample - loss: 1.3131\n",
      "Epoch 69/100\n",
      "6236/6236 [==============================] - 190s 30ms/sample - loss: 1.2818\n",
      "Epoch 70/100\n",
      "6236/6236 [==============================] - 189s 30ms/sample - loss: 1.2492\n",
      "Epoch 71/100\n",
      "6236/6236 [==============================] - 188s 30ms/sample - loss: 1.2227\n",
      "Epoch 72/100\n",
      "6236/6236 [==============================] - 181s 29ms/sample - loss: 1.1903\n",
      "Epoch 73/100\n",
      "6236/6236 [==============================] - 186s 30ms/sample - loss: 1.1618\n",
      "Epoch 74/100\n",
      "6236/6236 [==============================] - 193s 31ms/sample - loss: 1.1300\n",
      "Epoch 75/100\n",
      "6236/6236 [==============================] - 187s 30ms/sample - loss: 1.1075\n",
      "Epoch 76/100\n",
      "6236/6236 [==============================] - 186s 30ms/sample - loss: 1.0823\n",
      "Epoch 77/100\n",
      "6236/6236 [==============================] - 180s 29ms/sample - loss: 1.0502\n",
      "Epoch 78/100\n",
      "6236/6236 [==============================] - 189s 30ms/sample - loss: 1.0247\n",
      "Epoch 79/100\n",
      "6236/6236 [==============================] - 189s 30ms/sample - loss: 1.0008\n",
      "Epoch 80/100\n",
      "6236/6236 [==============================] - 189s 30ms/sample - loss: 0.9788\n",
      "Epoch 81/100\n",
      "6236/6236 [==============================] - 188s 30ms/sample - loss: 0.9551\n",
      "Epoch 82/100\n",
      "6236/6236 [==============================] - 179s 29ms/sample - loss: 0.9304\n",
      "Epoch 83/100\n",
      "6236/6236 [==============================] - 186s 30ms/sample - loss: 0.9070\n",
      "Epoch 84/100\n",
      "6236/6236 [==============================] - 190s 30ms/sample - loss: 0.8870\n",
      "Epoch 85/100\n",
      "6236/6236 [==============================] - 187s 30ms/sample - loss: 0.8661\n",
      "Epoch 86/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6236/6236 [==============================] - 190s 31ms/sample - loss: 0.8429\n",
      "Epoch 87/100\n",
      "6236/6236 [==============================] - 182s 29ms/sample - loss: 0.8267\n",
      "Epoch 88/100\n",
      "6236/6236 [==============================] - 181s 29ms/sample - loss: 0.8105\n",
      "Epoch 89/100\n",
      "6236/6236 [==============================] - 188s 30ms/sample - loss: 0.7914\n",
      "Epoch 90/100\n",
      "6236/6236 [==============================] - 194s 31ms/sample - loss: 0.7726\n",
      "Epoch 91/100\n",
      "6236/6236 [==============================] - 190s 31ms/sample - loss: 0.7556\n",
      "Epoch 92/100\n",
      "6236/6236 [==============================] - 184s 30ms/sample - loss: 0.7328\n",
      "Epoch 93/100\n",
      "6236/6236 [==============================] - 183s 29ms/sample - loss: 0.7196\n",
      "Epoch 94/100\n",
      "6236/6236 [==============================] - 189s 30ms/sample - loss: 0.7075\n",
      "Epoch 95/100\n",
      "6236/6236 [==============================] - 187s 30ms/sample - loss: 0.6866\n",
      "Epoch 96/100\n",
      "6236/6236 [==============================] - 190s 30ms/sample - loss: 0.6707\n",
      "Epoch 97/100\n",
      "6236/6236 [==============================] - 188s 30ms/sample - loss: 0.6551\n",
      "Epoch 98/100\n",
      "6236/6236 [==============================] - 180s 29ms/sample - loss: 0.6392\n",
      "Epoch 99/100\n",
      "6236/6236 [==============================] - 191s 31ms/sample - loss: 0.6244\n",
      "Epoch 100/100\n",
      "6236/6236 [==============================] - 189s 30ms/sample - loss: 0.6124\n"
     ]
    }
   ],
   "source": [
    "model.fit([encoder_input_data , decoder_input_data], decoder_output_data, batch_size=50, epochs=100 ) \n",
    "model.save( 'model14.h5' ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_inference_models():\n",
    "    \n",
    "    encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n",
    "    \n",
    "    decoder_state_input_h = tf.keras.layers.Input(shape=( 200 ,))\n",
    "    decoder_state_input_c = tf.keras.layers.Input(shape=( 200 ,))\n",
    "    \n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    \n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "        decoder_embedding , initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_model = tf.keras.models.Model(\n",
    "        [decoder_inputs] + decoder_states_inputs,\n",
    "        [decoder_outputs] + decoder_states)\n",
    "    \n",
    "    return encoder_model , decoder_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_tokens( sentence : str ):\n",
    "    words = sentence.lower().split()\n",
    "    tokens_list = list()\n",
    "    for word in words:\n",
    "        try:\n",
    "            tokens_list.append( tokenizer.word_index[ word ] ) \n",
    "        except KeyError as e:\n",
    "             print('Movie Bot: I am sorry I do not understand this word ' + word)\n",
    "    return preprocessing.sequence.pad_sequences( [tokens_list] , maxlen=maxlen_questions , padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie bot prompt : Hi how are you\n",
      "End\n",
      "Movie_bot:  not gonna gonna have to see me end\n",
      "Movie bot prompt : what do you mean\n",
      "End\n",
      "Movie_bot:  i mean is it is it end\n",
      "Movie bot prompt : Is that really what you mean\n",
      "End\n",
      "Movie_bot:  no end\n",
      "Movie bot prompt : excuse me\n",
      "End\n",
      "Movie_bot:  yes end\n",
      "Movie bot prompt : that does not make sense\n",
      "End\n",
      "Movie_bot:  what end\n",
      "Movie bot prompt : this is fun\n",
      "End\n",
      "Movie_bot:  yeah it sounds great end\n",
      "Movie bot prompt : ran out of time\n",
      "End\n",
      "Movie_bot:  that's what you going to tell me who i didn't want to know i don't want to know what it will this is better than you end\n",
      "Movie bot prompt : what\n",
      "End\n",
      "Movie_bot:  i can't end\n",
      "Movie bot prompt : I guess this is good\n",
      "End\n",
      "Movie_bot:  you and you know what's the name end\n",
      "Movie bot prompt : The wine guy\n",
      "End\n",
      "Movie_bot:  it got an actor so end\n"
     ]
    }
   ],
   "source": [
    "\n",
    "enc_model , dec_model = make_inference_models()\n",
    "\n",
    "\n",
    "for _ in range(10):\n",
    "    prompt = input( 'Movie bot prompt : ' )\n",
    "    if(prompt == 'end'):\n",
    "        break\n",
    "    states_values = enc_model.predict( str_to_tokens( prompt) )\n",
    "    empty_target_seq = np.zeros( ( 1 , 1 ) )\n",
    "    empty_target_seq[0, 0] = tokenizer.word_index['start']\n",
    "    stop_condition = False\n",
    "    decoded_translation = ''\n",
    "    while not stop_condition :\n",
    "        dec_outputs , h , c = dec_model.predict([ empty_target_seq ] + states_values )\n",
    "        sampled_word_index = np.argmax( dec_outputs[0, -1, :] )\n",
    "        sampled_word = None\n",
    "        for word , index in tokenizer.word_index.items() :\n",
    "            if sampled_word_index == index :\n",
    "                decoded_translation += ' {}'.format( word )\n",
    "                sampled_word = word\n",
    "                       \n",
    "        if sampled_word == \"end\" or prompt == 'end' or len(decoded_translation.split()) > maxlen_answers:\n",
    "            print(\"End\")\n",
    "            stop_condition = True\n",
    "            \n",
    "        empty_target_seq = np.zeros( ( 1 , 1 ) )  \n",
    "        empty_target_seq[ 0 , 0 ] = sampled_word_index\n",
    "        states_values = [ h , c ] \n",
    "\n",
    "    print( \"Movie_bot: \" + decoded_translation )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie bot prompt : Not from the real world\n",
      "End\n",
      "Movie_bot:  i know how it was supposed to be be writing adult world if you don't me me if i don't want to go if it's not feel my drop me a lot of mine i lead if you just were here i can i just will all come in my head and i knew you could she might be throw me again now end\n",
      "Movie bot prompt : only in my imagination\n",
      "End\n",
      "Movie_bot:  he's in love need too know they don't even want back back end\n",
      "Movie bot prompt : I am going to ask you to marry me\n",
      "End\n",
      "Movie_bot:  if you get it to you on the car before that you have most most of it for you end\n",
      "Movie bot prompt : does that mena we can not be lovers\n",
      "I am sorry I do not understand this word mena\n",
      "End\n",
      "Movie_bot:  then have you been in here and if them you could find you on the ass end\n",
      "Movie bot prompt : does that mean we can not be lovers\n",
      "End\n",
      "Movie_bot:  then can you give you in my world you're about the she out and you a little hope in case something he's interested in hear for the father and you had a mean if you got it if you can stay if you were the ring you can single go nuts friend end\n",
      "Movie bot prompt : yes continue\n",
      "I am sorry I do not understand this word continue\n",
      "End\n",
      "Movie_bot:  i don't know i just got a dog but if you said some other then we all then it is that your home is the problem it's not even something of get you it's in new york with feel a month end\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    prompt = input( 'Movie bot prompt : ' )\n",
    "    if(prompt == 'end'):\n",
    "        break\n",
    "    states_values = enc_model.predict( str_to_tokens( prompt) )\n",
    "    empty_target_seq = np.zeros( ( 1 , 1 ) )\n",
    "    empty_target_seq[0, 0] = tokenizer.word_index['start']\n",
    "    stop_condition = False\n",
    "    decoded_translation = ''\n",
    "    while not stop_condition :\n",
    "        dec_outputs , h , c = dec_model.predict([ empty_target_seq ] + states_values )\n",
    "        sampled_word_index = np.argmax( dec_outputs[0, -1, :] )\n",
    "        sampled_word = None\n",
    "        for word , index in tokenizer.word_index.items() :\n",
    "            if sampled_word_index == index :\n",
    "                decoded_translation += ' {}'.format( word )\n",
    "                sampled_word = word\n",
    "                       \n",
    "        if sampled_word == \"end\" or prompt == 'end' or len(decoded_translation.split()) > maxlen_answers:\n",
    "            print(\"End\")\n",
    "            stop_condition = True\n",
    "            \n",
    "        empty_target_seq = np.zeros( ( 1 , 1 ) )  \n",
    "        empty_target_seq[ 0 , 0 ] = sampled_word_index\n",
    "        states_values = [ h , c ] \n",
    "\n",
    "    print( \"Movie_bot: \" + decoded_translation )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie bot prompt : do you like baseball\n",
      "End\n",
      "Movie_bot:  yes i do end\n",
      "Movie bot prompt : is baseball fun\n",
      "End\n",
      "Movie_bot:  yeah well there's no you're no steve drink to know end\n",
      "Movie bot prompt : how is your imagination\n",
      "End\n",
      "Movie_bot:  she's been thinking about your hair end\n",
      "Movie bot prompt : what is it like being virtual\n",
      "I am sorry I do not understand this word virtual\n",
      "End\n",
      "Movie_bot:  the one was he said a good live in his date was a special and he haven't got a wazoo in miss my kid and he just out of you home and things home and if you it called me even a lot well you don't know my show you and what's your name and i am not my name end\n",
      "Movie bot prompt : virtual\n",
      "I am sorry I do not understand this word virtual\n",
      "End\n",
      "Movie_bot:  you know no you're not end\n",
      "Movie bot prompt : end\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    prompt = input( 'Movie bot prompt : ' )\n",
    "    if(prompt == 'end'):\n",
    "        break\n",
    "    states_values = enc_model.predict( str_to_tokens( prompt) )\n",
    "    empty_target_seq = np.zeros( ( 1 , 1 ) )\n",
    "    empty_target_seq[0, 0] = tokenizer.word_index['start']\n",
    "    stop_condition = False\n",
    "    decoded_translation = ''\n",
    "    while not stop_condition :\n",
    "        dec_outputs , h , c = dec_model.predict([ empty_target_seq ] + states_values )\n",
    "        sampled_word_index = np.argmax( dec_outputs[0, -1, :] )\n",
    "        sampled_word = None\n",
    "        for word , index in tokenizer.word_index.items() :\n",
    "            if sampled_word_index == index :\n",
    "                decoded_translation += ' {}'.format( word )\n",
    "                sampled_word = word\n",
    "                       \n",
    "        if sampled_word == \"end\" or prompt == 'end' or len(decoded_translation.split()) > maxlen_answers:\n",
    "            print(\"End\")\n",
    "            stop_condition = True\n",
    "            \n",
    "        empty_target_seq = np.zeros( ( 1 , 1 ) )  \n",
    "        empty_target_seq[ 0 , 0 ] = sampled_word_index\n",
    "        states_values = [ h , c ] \n",
    "\n",
    "    print( \"Movie_bot: \" + decoded_translation )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
