{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from tensorflow.keras import layers , activations , models , preprocessing\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB SIZE : 6709\n"
     ]
    }
   ],
   "source": [
    "r\"\"\" Build a word sequence-to-sequence training set \"\"\"\n",
    "\n",
    "from tensorflow.keras import preprocessing , utils\n",
    "\n",
    "questions = list()\n",
    "answers = list()\n",
    "\n",
    "with open(\"movies-sequence-input.txt\", \"r\") as file_input:\n",
    "    movie_input = file_input.read()\n",
    "df_input = pd.DataFrame(movie_input.split('\\n'),columns=list('i'))\n",
    "df_input = df_input.fillna(' ')\n",
    "\n",
    "with open(\"movies-sequence-output.txt\", \"r\") as file_output:\n",
    "    movie_output = file_output.read()\n",
    "df_output = pd.DataFrame(movie_output.split('\\n'),columns=list('o'))\n",
    "df_output = df_output.fillna(' ')\n",
    "\n",
    "for input_text, target_text in zip(df_input.i, df_output.o):\n",
    "    if(len(input_text)>400):\n",
    "        questions.append(input_text[:400])\n",
    "    else:\n",
    "        questions.append(input_text)\n",
    "    if(len(target_text)>400):\n",
    "        answers.append(target_text[:400])\n",
    "    else:\n",
    "        answers.append(target_text)\n",
    "answers_with_tags = list()\n",
    "for i in range( len( answers ) ):\n",
    "    if type( answers[i] ) == str:\n",
    "        answers_with_tags.append( answers[i] )\n",
    "    else:\n",
    "        questions.pop( i )\n",
    "\n",
    "answers = list()\n",
    "for i in range( len( answers_with_tags ) ) :\n",
    "    answers.append( '<START> ' + answers_with_tags[i] + ' <END>' )\n",
    "\n",
    "tokenizer = preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts( questions + answers )\n",
    "VOCAB_SIZE = len( tokenizer.word_index )+1\n",
    "print( 'VOCAB SIZE : {}'.format( VOCAB_SIZE ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5403, 81) 81\n",
      "(5403, 89) 89\n",
      "(5403, 89, 6709)\n"
     ]
    }
   ],
   "source": [
    "# encoder_input_data\n",
    "tokenized_questions = tokenizer.texts_to_sequences( questions )\n",
    "maxlen_questions = max( [ len(x) for x in tokenized_questions ] )\n",
    "padded_questions = preprocessing.sequence.pad_sequences( tokenized_questions , maxlen=maxlen_questions , padding='post' )\n",
    "encoder_input_data = np.array( padded_questions )\n",
    "print( encoder_input_data.shape , maxlen_questions )\n",
    "\n",
    "# decoder_input_data\n",
    "tokenized_answers = tokenizer.texts_to_sequences( answers )\n",
    "maxlen_answers = max( [ len(x) for x in tokenized_answers ] )\n",
    "padded_answers = preprocessing.sequence.pad_sequences( tokenized_answers , maxlen=maxlen_answers , padding='post' )\n",
    "decoder_input_data = np.array( padded_answers )\n",
    "print( decoder_input_data.shape , maxlen_answers )\n",
    "\n",
    "# decoder_output_data\n",
    "tokenized_answers = tokenizer.texts_to_sequences( answers )\n",
    "for i in range(len(tokenized_answers)) :\n",
    "    tokenized_answers[i] = tokenized_answers[i][1:]\n",
    "padded_answers = preprocessing.sequence.pad_sequences( tokenized_answers , maxlen=maxlen_answers , padding='post' )\n",
    "onehot_answers = utils.to_categorical( padded_answers , VOCAB_SIZE )\n",
    "decoder_output_data = np.array( onehot_answers )\n",
    "print( decoder_output_data.shape )\n",
    "\n",
    "# Saving all the arrays to storage\n",
    "np.save( 'enc_in_data.npy' , encoder_input_data )\n",
    "np.save( 'dec_in_data.npy' , decoder_input_data )\n",
    "np.save( 'dec_tar_data.npy' , decoder_output_data )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\jbark\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, None, 200)    1341800     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 200)    1341800     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 200), (None, 320800      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 200),  320800      embedding_1[0][0]                \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 6709)   1348509     lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 4,673,709\n",
      "Trainable params: 4,673,709\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_inputs = tf.keras.layers.Input(shape=( None , ))\n",
    "encoder_embedding = tf.keras.layers.Embedding( VOCAB_SIZE, 200 , mask_zero=True ) (encoder_inputs)\n",
    "encoder_outputs , state_h , state_c = tf.keras.layers.LSTM( 200 , return_state=True )( encoder_embedding )\n",
    "encoder_states = [ state_h , state_c ]\n",
    "\n",
    "decoder_inputs = tf.keras.layers.Input(shape=( None ,  ))\n",
    "decoder_embedding = tf.keras.layers.Embedding( VOCAB_SIZE, 200 , mask_zero=True) (decoder_inputs)\n",
    "decoder_lstm = tf.keras.layers.LSTM( 200 , return_state=True , return_sequences=True )\n",
    "decoder_outputs , _ , _ = decoder_lstm ( decoder_embedding , initial_state=encoder_states )\n",
    "decoder_dense = tf.keras.layers.Dense( VOCAB_SIZE , activation=tf.keras.activations.softmax ) \n",
    "output = decoder_dense ( decoder_outputs )\n",
    "\n",
    "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], output )\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='categorical_crossentropy')\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5403/5403 [==============================] - 148s 27ms/sample - loss: 5.7940\n",
      "Epoch 2/100\n",
      "5403/5403 [==============================] - 147s 27ms/sample - loss: 5.3624\n",
      "Epoch 3/100\n",
      "5403/5403 [==============================] - 146s 27ms/sample - loss: 5.1783\n",
      "Epoch 4/100\n",
      "5403/5403 [==============================] - 147s 27ms/sample - loss: 5.0505\n",
      "Epoch 5/100\n",
      "5403/5403 [==============================] - 143s 26ms/sample - loss: 4.9370\n",
      "Epoch 6/100\n",
      "5403/5403 [==============================] - 146s 27ms/sample - loss: 4.8316\n",
      "Epoch 7/100\n",
      "5403/5403 [==============================] - 150s 28ms/sample - loss: 4.7356\n",
      "Epoch 8/100\n",
      "5403/5403 [==============================] - 146s 27ms/sample - loss: 4.6466\n",
      "Epoch 9/100\n",
      "5403/5403 [==============================] - 146s 27ms/sample - loss: 4.5642\n",
      "Epoch 10/100\n",
      "5403/5403 [==============================] - 148s 27ms/sample - loss: 4.4883\n",
      "Epoch 11/100\n",
      "5403/5403 [==============================] - 147s 27ms/sample - loss: 4.4156\n",
      "Epoch 12/100\n",
      "5403/5403 [==============================] - 145s 27ms/sample - loss: 4.3411\n",
      "Epoch 13/100\n",
      "5403/5403 [==============================] - 145s 27ms/sample - loss: 4.2703\n",
      "Epoch 14/100\n",
      "5403/5403 [==============================] - 144s 27ms/sample - loss: 4.1931\n",
      "Epoch 15/100\n",
      "5403/5403 [==============================] - 147s 27ms/sample - loss: 4.1236\n",
      "Epoch 16/100\n",
      "5403/5403 [==============================] - 145s 27ms/sample - loss: 4.0545\n",
      "Epoch 17/100\n",
      "5403/5403 [==============================] - 145s 27ms/sample - loss: 3.9850\n",
      "Epoch 18/100\n",
      "5403/5403 [==============================] - 148s 27ms/sample - loss: 3.9111\n",
      "Epoch 19/100\n",
      "5403/5403 [==============================] - 147s 27ms/sample - loss: 3.8426\n",
      "Epoch 20/100\n",
      "5403/5403 [==============================] - 144s 27ms/sample - loss: 3.7664\n",
      "Epoch 21/100\n",
      "5403/5403 [==============================] - 148s 27ms/sample - loss: 3.7009\n",
      "Epoch 22/100\n",
      "5403/5403 [==============================] - 147s 27ms/sample - loss: 3.6295\n",
      "Epoch 23/100\n",
      "5403/5403 [==============================] - 149s 28ms/sample - loss: 3.5605\n",
      "Epoch 24/100\n",
      "5403/5403 [==============================] - 149s 27ms/sample - loss: 3.4920\n",
      "Epoch 25/100\n",
      "5403/5403 [==============================] - 150s 28ms/sample - loss: 3.4259\n",
      "Epoch 26/100\n",
      "5403/5403 [==============================] - 146s 27ms/sample - loss: 3.3562\n",
      "Epoch 27/100\n",
      "5403/5403 [==============================] - 143s 26ms/sample - loss: 3.2900\n",
      "Epoch 28/100\n",
      "5403/5403 [==============================] - 151s 28ms/sample - loss: 3.2203\n",
      "Epoch 29/100\n",
      "5403/5403 [==============================] - 150s 28ms/sample - loss: 3.1546\n",
      "Epoch 30/100\n",
      "5403/5403 [==============================] - 148s 27ms/sample - loss: 3.0856\n",
      "Epoch 31/100\n",
      "5403/5403 [==============================] - 148s 27ms/sample - loss: 3.0221\n",
      "Epoch 32/100\n",
      "5403/5403 [==============================] - 148s 27ms/sample - loss: 2.9552\n",
      "Epoch 33/100\n",
      "5403/5403 [==============================] - 144s 27ms/sample - loss: 2.8905\n",
      "Epoch 34/100\n",
      "5403/5403 [==============================] - 149s 28ms/sample - loss: 2.8187\n",
      "Epoch 35/100\n",
      "5403/5403 [==============================] - 150s 28ms/sample - loss: 2.7590\n",
      "Epoch 36/100\n",
      "5403/5403 [==============================] - 148s 27ms/sample - loss: 2.6949\n",
      "Epoch 37/100\n",
      "5403/5403 [==============================] - 151s 28ms/sample - loss: 2.6340\n",
      "Epoch 38/100\n",
      "5403/5403 [==============================] - 151s 28ms/sample - loss: 2.5748\n",
      "Epoch 39/100\n",
      "5403/5403 [==============================] - 148s 27ms/sample - loss: 2.5178\n",
      "Epoch 40/100\n",
      "5403/5403 [==============================] - 145s 27ms/sample - loss: 2.4579\n",
      "Epoch 41/100\n",
      "5403/5403 [==============================] - 147s 27ms/sample - loss: 2.3966\n",
      "Epoch 42/100\n",
      "5403/5403 [==============================] - 149s 28ms/sample - loss: 2.3377\n",
      "Epoch 43/100\n",
      "5403/5403 [==============================] - 148s 27ms/sample - loss: 2.2826\n",
      "Epoch 44/100\n",
      "5403/5403 [==============================] - 149s 27ms/sample - loss: 2.2276\n",
      "Epoch 45/100\n",
      "5403/5403 [==============================] - 149s 28ms/sample - loss: 2.1778\n",
      "Epoch 46/100\n",
      "5403/5403 [==============================] - 145s 27ms/sample - loss: 2.1193\n",
      "Epoch 47/100\n",
      "5403/5403 [==============================] - 146s 27ms/sample - loss: 2.0614\n",
      "Epoch 48/100\n",
      "5403/5403 [==============================] - 149s 28ms/sample - loss: 2.0086\n",
      "Epoch 49/100\n",
      "5403/5403 [==============================] - 152s 28ms/sample - loss: 1.9553\n",
      "Epoch 50/100\n",
      "5403/5403 [==============================] - 148s 27ms/sample - loss: 1.9022\n",
      "Epoch 51/100\n",
      "5403/5403 [==============================] - 148s 27ms/sample - loss: 1.8517\n",
      "Epoch 52/100\n",
      "5403/5403 [==============================] - 151s 28ms/sample - loss: 1.7983\n",
      "Epoch 53/100\n",
      "5403/5403 [==============================] - 146s 27ms/sample - loss: 1.7473\n",
      "Epoch 54/100\n",
      "5403/5403 [==============================] - 153s 28ms/sample - loss: 1.6977\n",
      "Epoch 55/100\n",
      "5403/5403 [==============================] - 149s 28ms/sample - loss: 1.6505\n",
      "Epoch 56/100\n",
      "5403/5403 [==============================] - 151s 28ms/sample - loss: 1.6016\n",
      "Epoch 57/100\n",
      "5403/5403 [==============================] - 148s 27ms/sample - loss: 1.5553\n",
      "Epoch 58/100\n",
      "5403/5403 [==============================] - 149s 28ms/sample - loss: 1.5112\n",
      "Epoch 59/100\n",
      "5403/5403 [==============================] - 144s 27ms/sample - loss: 1.4680\n",
      "Epoch 60/100\n",
      "5403/5403 [==============================] - 146s 27ms/sample - loss: 1.4272\n",
      "Epoch 61/100\n",
      "5403/5403 [==============================] - 152s 28ms/sample - loss: 1.3825\n",
      "Epoch 62/100\n",
      "5403/5403 [==============================] - 150s 28ms/sample - loss: 1.3448\n",
      "Epoch 63/100\n",
      "5403/5403 [==============================] - 150s 28ms/sample - loss: 1.3050\n",
      "Epoch 64/100\n",
      "5403/5403 [==============================] - 146s 27ms/sample - loss: 1.2649\n",
      "Epoch 65/100\n",
      "5403/5403 [==============================] - 151s 28ms/sample - loss: 1.2266\n",
      "Epoch 66/100\n",
      "5403/5403 [==============================] - 145s 27ms/sample - loss: 1.1897\n",
      "Epoch 67/100\n",
      "5403/5403 [==============================] - 146s 27ms/sample - loss: 1.1539\n",
      "Epoch 68/100\n",
      "5403/5403 [==============================] - 151s 28ms/sample - loss: 1.1192\n",
      "Epoch 69/100\n",
      "5403/5403 [==============================] - 151s 28ms/sample - loss: 1.0853\n",
      "Epoch 70/100\n",
      "5403/5403 [==============================] - 150s 28ms/sample - loss: 1.0525\n",
      "Epoch 71/100\n",
      "5403/5403 [==============================] - 150s 28ms/sample - loss: 1.0166\n",
      "Epoch 72/100\n",
      "5403/5403 [==============================] - 146s 27ms/sample - loss: 0.9870\n",
      "Epoch 73/100\n",
      "5403/5403 [==============================] - 144s 27ms/sample - loss: 0.9583\n",
      "Epoch 74/100\n",
      "5403/5403 [==============================] - 150s 28ms/sample - loss: 0.9286\n",
      "Epoch 75/100\n",
      "5403/5403 [==============================] - 147s 27ms/sample - loss: 0.9002\n",
      "Epoch 76/100\n",
      "5403/5403 [==============================] - 149s 28ms/sample - loss: 0.8664\n",
      "Epoch 77/100\n",
      "5403/5403 [==============================] - 146s 27ms/sample - loss: 0.8417\n",
      "Epoch 78/100\n",
      "5403/5403 [==============================] - 150s 28ms/sample - loss: 0.8123\n",
      "Epoch 79/100\n",
      "5403/5403 [==============================] - 145s 27ms/sample - loss: 0.7889\n",
      "Epoch 80/100\n",
      "5403/5403 [==============================] - 147s 27ms/sample - loss: 0.7640\n",
      "Epoch 81/100\n",
      "5403/5403 [==============================] - 153s 28ms/sample - loss: 0.7364\n",
      "Epoch 82/100\n",
      "5403/5403 [==============================] - 149s 28ms/sample - loss: 0.7102\n",
      "Epoch 83/100\n",
      "5403/5403 [==============================] - 151s 28ms/sample - loss: 0.6865\n",
      "Epoch 84/100\n",
      "5403/5403 [==============================] - 151s 28ms/sample - loss: 0.6620\n",
      "Epoch 85/100\n",
      "5403/5403 [==============================] - 151s 28ms/sample - loss: 0.6426\n",
      "Epoch 86/100\n",
      "5403/5403 [==============================] - 147s 27ms/sample - loss: 0.6186\n",
      "Epoch 87/100\n",
      "5403/5403 [==============================] - 146s 27ms/sample - loss: 0.6001\n",
      "Epoch 88/100\n",
      "5403/5403 [==============================] - 144s 27ms/sample - loss: 0.5809\n",
      "Epoch 89/100\n",
      "5403/5403 [==============================] - 147s 27ms/sample - loss: 0.5554\n",
      "Epoch 90/100\n",
      "5403/5403 [==============================] - 148s 27ms/sample - loss: 0.5400\n",
      "Epoch 91/100\n",
      "5403/5403 [==============================] - 148s 27ms/sample - loss: 0.5184\n",
      "Epoch 92/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5403/5403 [==============================] - 152s 28ms/sample - loss: 0.5040\n",
      "Epoch 93/100\n",
      "5403/5403 [==============================] - 148s 27ms/sample - loss: 0.4867\n",
      "Epoch 94/100\n",
      "5403/5403 [==============================] - 146s 27ms/sample - loss: 0.4672\n",
      "Epoch 95/100\n",
      "5403/5403 [==============================] - 144s 27ms/sample - loss: 0.4510\n",
      "Epoch 96/100\n",
      "5403/5403 [==============================] - 150s 28ms/sample - loss: 0.4346\n",
      "Epoch 97/100\n",
      "5403/5403 [==============================] - 151s 28ms/sample - loss: 0.4183\n",
      "Epoch 98/100\n",
      "5403/5403 [==============================] - 149s 28ms/sample - loss: 0.4059\n",
      "Epoch 99/100\n",
      "5403/5403 [==============================] - 146s 27ms/sample - loss: 0.3907\n",
      "Epoch 100/100\n",
      "5403/5403 [==============================] - 148s 27ms/sample - loss: 0.3748\n"
     ]
    }
   ],
   "source": [
    "model.fit([encoder_input_data , decoder_input_data], decoder_output_data, batch_size=50, epochs=100 ) \n",
    "model.save( 'model3.h5' ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_inference_models():\n",
    "    \n",
    "    encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n",
    "    \n",
    "    decoder_state_input_h = tf.keras.layers.Input(shape=( 200 ,))\n",
    "    decoder_state_input_c = tf.keras.layers.Input(shape=( 200 ,))\n",
    "    \n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    \n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "        decoder_embedding , initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_model = tf.keras.models.Model(\n",
    "        [decoder_inputs] + decoder_states_inputs,\n",
    "        [decoder_outputs] + decoder_states)\n",
    "    \n",
    "    return encoder_model , decoder_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_tokens( sentence : str ):\n",
    "    words = sentence.lower().split()\n",
    "    tokens_list = list()\n",
    "    for word in words:\n",
    "        tokens_list.append( tokenizer.word_index[ word ] ) \n",
    "    return preprocessing.sequence.pad_sequences( [tokens_list] , maxlen=maxlen_questions , padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie bot prompt : Hi how are you\n",
      "End\n",
      "Movie_bot:  oh no end\n",
      "Movie bot prompt : what do you mean\n",
      "End\n",
      "Movie_bot:  i mean i'm a fucking fraud i'm no architect don't be a putz who's been to santiago twice in a year estadio olimpico please end\n",
      "Movie bot prompt : is that really what you mean\n",
      "End\n",
      "Movie_bot:  of course i mean i mean i was now i was the one end\n",
      "Movie bot prompt : excuse me\n",
      "End\n",
      "Movie_bot:  yes end\n",
      "Movie bot prompt : that does not make sense\n",
      "End\n",
      "Movie_bot:  you do have how about me okay end\n",
      "Movie bot prompt : this is fun\n",
      "End\n",
      "Movie_bot:  yeah i'm thinking of it didn't you wanted to look the girl who will be of an audition five but don't you know you're not going to run away from making the heart and end\n",
      "Movie bot prompt : ran out of time\n",
      "End\n",
      "Movie_bot:  a little would be so be going to be having a willing with a loaded deal are you decided to go in the priest end\n",
      "Movie bot prompt : what\n",
      "End\n",
      "Movie_bot:  you heard me you were right now end\n",
      "Movie bot prompt : I guess this is good\n",
      "End\n",
      "Movie_bot:  oh no no look not the wine name of broke by the guy end\n",
      "Movie bot prompt : The wine guy\n",
      "End\n",
      "Movie_bot:  yes well you're being dinner for her business business guys guys all good little time to guys are fine end\n"
     ]
    }
   ],
   "source": [
    "\n",
    "enc_model , dec_model = make_inference_models()\n",
    "\n",
    "\n",
    "for _ in range(10):\n",
    "    prompt = input( 'Movie bot prompt : ' )\n",
    "    if(prompt == 'end'):\n",
    "        break\n",
    "    states_values = enc_model.predict( str_to_tokens( prompt) )\n",
    "    empty_target_seq = np.zeros( ( 1 , 1 ) )\n",
    "    empty_target_seq[0, 0] = tokenizer.word_index['start']\n",
    "    stop_condition = False\n",
    "    decoded_translation = ''\n",
    "    while not stop_condition :\n",
    "        dec_outputs , h , c = dec_model.predict([ empty_target_seq ] + states_values )\n",
    "        sampled_word_index = np.argmax( dec_outputs[0, -1, :] )\n",
    "        sampled_word = None\n",
    "        for word , index in tokenizer.word_index.items() :\n",
    "            if sampled_word_index == index :\n",
    "                decoded_translation += ' {}'.format( word )\n",
    "                sampled_word = word\n",
    "                       \n",
    "        if sampled_word == \"end\" or prompt == 'end' or len(decoded_translation.split()) > maxlen_answers:\n",
    "            print(\"End\")\n",
    "            stop_condition = True\n",
    "            \n",
    "        empty_target_seq = np.zeros( ( 1 , 1 ) )  \n",
    "        empty_target_seq[ 0 , 0 ] = sampled_word_index\n",
    "        states_values = [ h , c ] \n",
    "\n",
    "    print( \"Movie_bot: \" + decoded_translation )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "respond('Hi Rosa, how are you?')\n",
    "respond('Hi Jim, how are you?')\n",
    "respond('Hi Barak, how are you?')\n",
    "respond('Hi Amy, how are you?')\n",
    "respond('Hi Paris, how are you?')\n",
    "respond('Hi Joe, how are you?')\n",
    "respond('Hi Jane, how are you?')\n",
    "respond('Hey Jane, how are you?')\n",
    "respond('Hey Jon, how are you?')\n",
    "respond('Hey John, how are you?')\n",
    "respond('Hey Joe, how are you?')\n",
    "respond('Hey Jim, how are you?')\n",
    "respond('Hey Ashley, how are you?')\n",
    "respond('Hey my love, how are you?')\n",
    "respond('Hey Arzu, how are you?')\n",
    "respond(\"I'm talking about us.\")\n",
    "respond(\"What are you trying to say?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
